{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Cleaning our Movie Data\n",
    "We'll download our list of movies, the movie's descriptions first from [TMDB](https://www.themoviedb.org/?language=en-US). Then do some cleaning of the data. Download [word2vec](https://en.wikipedia.org/wiki/Word2vec) and finally, do some feature engineering to tokenize the descriptions. \n",
    "\n",
    "We've split these into 6 seperate scripts and linked them all in one notebook for simplicity. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import boto3\n",
    "import io\n",
    "import pandas as pd\n",
    "from src.utils.initialize import *\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the list of Movies from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the credentials to access this s3 bucket stored in our project environment variables (AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY)\n",
    "# note that in reality these would likely either be in our user environment variables or we would use AWS credential propagation to pull these in automatically from you SSO \n",
    "client = boto3.client('s3')\n",
    "\n",
    "# download no_duplicate_movies.pkl from se-demo-bucket and write locally - this is our pull of data movies with some deduplication applied\n",
    "file = client.download_file('se-demo-bucket', 'movie-demo/data/interim/no_duplicate_movies.pkl', 'data/S3/interim/no_duplicate_movies.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all the Movie descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load no_duplicate_movies\n",
    "with open('data/S3/interim/no_duplicate_movies.pkl','rb') as f:\n",
    "    no_duplicate_movies=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset with of movies with overviews\n",
    "movies_with_overviews=[] # from poster data\n",
    "for i in range(len(no_duplicate_movies)):\n",
    "    movie=no_duplicate_movies[i]\n",
    "    id=movie['id']\n",
    "    overview=movie['overview']\n",
    "    \n",
    "    if len(overview)==0:\n",
    "        continue\n",
    "    else:\n",
    "        movies_with_overviews.append(movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1689"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movies_with_overviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/S3/interim/movies_with_overviews.pkl','wb') as f:\n",
    "    pickle.dump(movies_with_overviews,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tHere is the first entry in movies_with_overviews:\n",
      "{   'adult': False,\n",
      "    'backdrop_path': '/6O0lsCK90jZCyPvyYSt8Szzlnd6.jpg',\n",
      "    'genre_ids': [18, 36, 10752],\n",
      "    'id': 324786,\n",
      "    'original_language': 'en',\n",
      "    'original_title': 'Hacksaw Ridge',\n",
      "    'overview': 'WWII American Army Medic Desmond T. Doss, who served during '\n",
      "                'the Battle of Okinawa, refuses to kill people and becomes the '\n",
      "                'first Conscientious Objector in American history to receive '\n",
      "                'the Congressional Medal of Honor.',\n",
      "    'popularity': 89.492,\n",
      "    'poster_path': '/jhWbYeUNOA5zAb6ufK6pXQFXqTX.jpg',\n",
      "    'release_date': '2016-10-07',\n",
      "    'title': 'Hacksaw Ridge',\n",
      "    'video': False,\n",
      "    'vote_average': 8.1,\n",
      "    'vote_count': 9353}\n"
     ]
    }
   ],
   "source": [
    "print('\\tHere is the first entry in movies_with_overviews:')\n",
    "pprint.pprint(movies_with_overviews[0], indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and de-duping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the list of movies that have overviews from data/interim/movies_with_overviews.pkl.\n",
      "\n",
      "Extracting the genres and movie ids in prep for binarizination...\n",
      "Binarizing the list of genres to create the target variable Y.\n",
      "Done! Y created. Shape of Y is \n",
      "(1689, 19)\n",
      "\n",
      "\n",
      "Creating a mapping from the genre ids to the genre names...\n",
      "Mapping from genre id to genre name is saved in the Genre_ID_to_name dictionary:\n",
      "{   12: 'Adventure',\n",
      "    14: 'Fantasy',\n",
      "    16: 'Animation',\n",
      "    18: 'Drama',\n",
      "    27: 'Horror',\n",
      "    28: 'Action',\n",
      "    35: 'Comedy',\n",
      "    36: 'History',\n",
      "    37: 'Western',\n",
      "    53: 'Thriller',\n",
      "    80: 'Crime',\n",
      "    99: 'Documentary',\n",
      "    878: 'Science Fiction',\n",
      "    9648: 'Mystery',\n",
      "    10402: 'Music',\n",
      "    10749: 'Romance',\n",
      "    10751: 'Family',\n",
      "    10752: 'War',\n",
      "    10770: 'TV Movie'}\n",
      "\n",
      "\n",
      "Saved the mapping from genre id to genre name as data/processed/Genredict.pkl.\n",
      "Saved the target variable Y to data/processed/Y.pkl.\n",
      "\n",
      "\tHere are the first few lines of Y:\n",
      "\t[[0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0]\n",
      " [1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "run src/data/cleaning_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featuring engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import random\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Note: spark config already embedded in Domino Compute Environment, for seamless use...\n",
    " \n",
    "# Create the Spark Context\n",
    "sc=SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sys.argv) == 2:\n",
    "    #note: must be real number between 0 and 1\n",
    "    NUM_SAMPLES = int(sys.argv[1])\n",
    "else: \n",
    "    NUM_SAMPLES = 50000000\n",
    " \n",
    "def inside(p):\n",
    " x, y = random.random(), random.random()\n",
    " return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)).filter(inside).count()\n",
    "output = float(4.0 * count / float(NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the target variable from to data/processed/Y.pkl.\n",
      "\n",
      "Loaded the list of de-duped movies with overviews from data/interim/movies_with_overviews.pkl.\n",
      "Loaded the mapping from genre id to genre name from data/processed/Genredict.pkl.\n",
      "Removed punctuation from the overviews.\n",
      "Vectorized the text of the overviews using the CountVectorizer from scikit-learn. This is basically the bag of words model.\n",
      "\tShape of X with count vectorizer:\n",
      "\t(1689, 1232)\n",
      "\tSaved X to data/processed/X.pkl and the vectorizer as models/count_vectorizer.pkl.\n",
      "\tHere are the first row of X (remember that it is a sparse matrix):\n",
      "\t   (0, 1224)\t1\n",
      "  (0, 43)\t2\n",
      "  (0, 60)\t1\n",
      "  (0, 1192)\t1\n",
      "  (0, 313)\t1\n",
      "  (0, 1062)\t3\n",
      "  (0, 89)\t1\n",
      "  (0, 745)\t2\n",
      "  (0, 1087)\t2\n",
      "  (0, 585)\t1\n",
      "  (0, 788)\t1\n",
      "  (0, 47)\t1\n",
      "  (0, 96)\t1\n",
      "  (0, 405)\t1\n",
      "  (0, 539)\t1\n",
      "  (0, 507)\t1\n",
      "Vectorized the text of the overviews using the TfidfVectorizer from scikit-learn.\n",
      "\tShape of X with TF-IDF vectorizer:\n",
      "\t(1689, 1232)\n",
      "\tSaved X_tfidf to data/processed/X_tfidf.pkl and the vectorizer as models/tfidf_transformer.pkl.\n",
      "\tHere are the first row of X_tfidf (remember that it is as sparse matrix):\n",
      "\t   (0, 1224)\t0.3517170285152859\n",
      "  (0, 1192)\t0.1406284793774879\n",
      "  (0, 1087)\t0.1467824566476443\n",
      "  (0, 1062)\t0.19691877537353508\n",
      "  (0, 788)\t0.24742512566105895\n",
      "  (0, 745)\t0.154796183910727\n",
      "  (0, 585)\t0.3037617084920676\n",
      "  (0, 539)\t0.08928617398744622\n",
      "  (0, 507)\t0.28201586753221736\n",
      "  (0, 405)\t0.224584735043649\n",
      "  (0, 313)\t0.2206964493565\n",
      "  (0, 96)\t0.2387939679504262\n",
      "  (0, 89)\t0.2500154456977699\n",
      "  (0, 60)\t0.2616865298865321\n",
      "  (0, 47)\t0.07529171307729504\n",
      "  (0, 43)\t0.48827787322476357\n"
     ]
    }
   ],
   "source": [
    "run src/features/feature_eng.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the SLIMMED word2vec model...\n",
      "--2021-03-11 16:06:48--  https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz\n",
      "Resolving github.com (github.com)... 192.30.255.112\n",
      "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://media.githubusercontent.com/media/eyaler/word2vec-slim/master/GoogleNews-vectors-negative300-SLIM.bin.gz [following]\n",
      "--2021-03-11 16:06:48--  https://media.githubusercontent.com/media/eyaler/word2vec-slim/master/GoogleNews-vectors-negative300-SLIM.bin.gz\n",
      "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 276467217 (264M) [application/octet-stream]\n",
      "Saving to: ‘GoogleNews-vectors-negative300-SLIM.bin.gz’\n",
      "\n",
      "GoogleNews-vectors- 100%[===================>] 263.66M   199MB/s    in 1.3s    \n",
      "\n",
      "2021-03-11 16:06:56 (199 MB/s) - ‘GoogleNews-vectors-negative300-SLIM.bin.gz’ saved [276467217/276467217]\n",
      "\n",
      "Decompressing the model...\n",
      "GoogleNews-vectors-negative300-SLIM.bin.gz:\t 23.6% -- replaced with GoogleNews-vectors-negative300-SLIM.bin\n",
      "Decompressed. Moving...\n",
      "Move completed.\n"
     ]
    }
   ],
   "source": [
    "!sh src/models/get_word2vec.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering using [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the list of de-duped movies with overviews from data/interim/movies_with_overviews.pkl.\n",
      "Loaded the GoogleNews Slimmed Word2Vec model.\n",
      "Tokenized all overviews.\n",
      "Removed stopwords.\n",
      "Calculated the mean word2vec vector for each overview.\n",
      "Created a multi-label binarizer for genres.\n",
      "Transformed the target variable for each movie using the multi-label binarizer to an array or arrays.\n",
      "\tFor a movie with genre ids [36, 53, 10752], we create Y for the movie as [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0].\n",
      "Saved the mean word2vec vector for each overview (X) and the binarized target (Y) as textual_features=(X,Y) into data/processed/textual_features.pkl.\n",
      "Saved the multi-label binarizer so we can do the inverse transform later as models/mlb.pkl.\n"
     ]
    }
   ],
   "source": [
    "run src/features/word2vec_features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
